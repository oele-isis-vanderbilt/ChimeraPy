# Built-in Imports
from typing import Dict, Any
import multiprocessing as mp
import queue
import threading
import collections
import json
import pathlib
import time
import os
import queue
import signal

# Third-party imports

# ChimeraPy Library
from chimerapy.utils.tools import threaded
from chimerapy.core.queue import PortableQueue
from chimerapy.utils.memory_manager import MemoryManager
from chimerapy.core.video import VideoEntry
from chimerapy.core.tabular import TabularEntry, ImageEntry
from chimerapy.core.process import Process

# Resource:
# https://stackoverflow.com/questions/8489684/python-subclassing-multiprocessing-process

class Writer(Process):
    """Subprocess tasked with logging in an annotated and organized manner.

    The ``Logger`` focuses on logging data chunks, while keeping record
    of meta data for easy reconstruction of data processing throughout 
    the pipeline. The ``Logger`` is primarly used in the forward 
    propagation of the data through the pipeline.

    """

    def __init__(
            self,
            logdir:pathlib.Path,
            experiment_name:str,
            memory_manager:MemoryManager
        ):
        """Construct a ``Logger`` to log data in a structured manner.

        Args:
            logdir (pathlib.Path): The log directory to save the folder \
                generated by the ``Logger`` instance.

            experiment_name (str): The name of the experiment, which \
                will be used to create a unique folder for the ``Logger``.

        """
        super().__init__(
            name=self.__class__.__name__,
            inputs=None
        )
        
        # Save the input parameters
        self.logdir = logdir
        self.experiment_name = experiment_name
        self.experiment_dir = self.logdir / self.experiment_name
        self.memory_manager = memory_manager

        # Keeping records of all logged data
        self.records = collections.defaultdict(dict)
        self.threads = collections.defaultdict(dict)
        self.thread_queues = collections.defaultdict(dict)
        self.meta_data = {}
        self.dtype_to_class = {
            'tabular': TabularEntry,
            'image': ImageEntry,
            'video': VideoEntry
        }
        
        # Create a lock to prevent multiple threads from executing 
        # interferring code at the same time
        self.lock = threading.Lock()
        
        # Create the folder if it doesn't exist 
        if not self.logdir.exists():
            os.mkdir(self.logdir)

        # Create the experiment dir
        if not self.experiment_dir.exists():
            os.mkdir(self.experiment_dir)
        
        # Create a JSON file with the session's meta
        self.meta_data = {
            'id': self.experiment_name, 
            'subsessions': [], 
            'records': collections.defaultdict(dict)
        }
        self._save_meta_data()

        # Adding specific function class from the message
        self.subclass_message_to_functions.update({
        })

    def message_logging_status(self, data_chunk):
        """message_from function to report the number of logged data."""

        # Create the message
        logging_status_message = {
            'header': 'UPDATE',
            'body': {
                'type': 'COUNTER',
                'content': {
                    'uuid': data_chunk['uuid'],
                    'num_of_logged_data': self.num_of_logged_data,
                }
            }
        }

        # Send the message
        try:
            self.message_from_queue.put(logging_status_message.copy(), timeout=0.5)
        except queue.Full:
            print("Logging_status_message failed to send!")

    def message_logger_finished(self):
        """message_from function to report logging completion."""

        # Create the message
        logging_status_message = {
            'header': 'META',
            'body': {
                'type': 'END',
                'content': {}
            }
        }

        # Send the message
        try:
            self.message_from_queue.put(logging_status_message.copy(), timeout=0.5)
        except queue.Full:
            print("Logging_status_message failed to send!")
    
    def _save_meta_data(self):
        """Save the meta to a JSON file."""
        # Added lock to prevent interfering in writing data in shared file
        self.lock.acquire()
        with open(self.experiment_dir / 'meta.json', "w") as json_file:
            json.dump(self.meta_data, json_file)
        self.lock.release()

    def create_entry(self, data_chunk:Dict[str, Any]):
        """Create an entry and add it to the records and meta data.

        Args:
            data_chunk (Dict[str, Any]): The data chunk that details the \
            data type and name for the entry.

        """
        # New session processing (get the entry's directory)
        if data_chunk['session_name'] == 'root':
            entry_dir = self.experiment_dir

        else:# Add the new session to the subsesssions list
            if data_chunk['session_name'] not in self.meta_data['subsessions']:
                self.meta_data['subsessions'].append(data_chunk['session_name'])
            entry_dir = self.experiment_dir / data_chunk['session_name']

        # Selecting the class
        entry_cls = self.dtype_to_class[data_chunk['dtype']]

        # Creating the entry and recording in meta data
        self.records[data_chunk['session_name']][data_chunk['name']] = entry_cls(entry_dir, data_chunk['name'])
        entry_meta_data = {
            'dtype': data_chunk['dtype'],
            'start_time': str(data_chunk['data'].iloc[0]._time_),
            'end_time': str(data_chunk['data'].iloc[-1]._time_),
        }
        self.meta_data['records'][data_chunk['session_name']][data_chunk['name']] = entry_meta_data
        self._save_meta_data()
  
    @threaded
    def entry_thread(self, queue:queue.Queue):
        """Create a new thread for an entry of data.

        Each thread has the individual responsibility of update a specific 
        entry. The input queue is the matching data flow for said entry.

        Args:
            queue (queue.Queue): The queue that feeds data to the thread.

        """
        # Continue processing
        while self.running.value:

            # If we have data to log, work on it
            if queue.qsize() != 0:
                
                # Log the data
                data_chunk = queue.get()
                self.flush(data_chunk)

                # Notify that the data is complete!
                self.num_of_logged_data += 1
                self.message_logging_status(data_chunk)

            # Else, sleep to save resources
            else:
                time.sleep(0.1)

            # Breaking condition
            if queue.qsize() == 0:
                break
   
    def flush(self, data:Dict[str, Any]):
        """Flush out unsaved logged changes by saving and clearing cache.

        This function saves the logged changes, where each logged 
        information is attached to an ``Entry`` and its name. All the 
        logged data with the same ``entry_name`` will be stored together.
        If the logged data is the first time saved, there are additional
        preparation required (such as opening files, creating directories,
        and creating ``Entry`` instances). Else, the logged data is 
        appended to the ``Entry``.

        Args:
            data (Dict[str, Any]): The data chuck containing ``uuid``, \
                ``session_name``, ``name``, ``data``, and ``dtype`` keys. \
                This information fully describes the content inside the \
                ``data``.

        """
        # Test that the new data entry is valid to the type of entry
        assert isinstance(self.records[data['session_name']][data['name']], self.dtype_to_class[data['dtype']]), \
            f"Entry Type={self.records[data['session_name']][data['name']]} should match input data dtype {data['dtype']}"

        # Need to update the end_time for meta_data
        if len(data['data']) > 0:
            end_time_stamp = str(data['data'].iloc[-1]._time_)
            self.meta_data['records'][data['session_name']][data['name']]['end_time'] = end_time_stamp 
            self._save_meta_data()

        # Now that we have account for both scenarios, just log data!
        self.records[data['session_name']][data['name']].append(data)
        self.records[data['session_name']][data['name']].flush()

    def setup(self) -> None:
        
        # Ignore SIGINT signal
        signal.signal(signal.SIGINT, signal.SIG_IGN)

        # Keeping track of processed data
        self.num_of_logged_data = 0

    def step(self, data_chunk:Dict[str, Any]) -> None:

        # Extract the session name and entry name
        session_name = data_chunk['session_name']
        entry_name = data_chunk['name']

        # Determine if the data chunk is for a new entry, if so,
        # then create a new thread and pass that data!
        if session_name not in self.records.keys() or entry_name not in self.records[session_name].keys():
           
            # Create the entry for the data_chunk
            self.create_entry(data_chunk)

            # Setup the thread with its queue
            new_entry_queue = queue.Queue() 
            new_entry_thread = self.entry_thread(queue=new_entry_queue)

            # Start the thread
            new_entry_thread.start()

            # Store them
            self.thread_queues[session_name][entry_name] = new_entry_queue
            self.threads[session_name][entry_name] = new_entry_thread

        # Now that we have ensure that a thread exists, feed the
        # data chunk to that thread
        self.thread_queues[session_name][entry_name].put(data_chunk)

    def teardown(self):
        super().teardown()

        # Wait until all the logging threads have stopped!
        for session_name in self.threads.keys():
            for entry_name in self.threads[session_name].keys():
                thread = self.threads[session_name][entry_name]
                thread.join()
        
        # Then close all the entries
        for session in self.records.values():
            for entry in session.values():
                entry.close()

        # Sending message that the Logger finished!
        self.message_logger_finished()
