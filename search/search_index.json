{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyMMDT Welcome to PyMMDT documentation! The GO-TO spot for easy and modular multimodal analysis and visualization. What is PyMMDT? The objective of PyMMDT library is to make multimodal data collection, processing, and visualization for both the offline and online setting an easier task. Our modular structure helps make this library flexible to be molded and shaped to your project's needs. PyMDDT General Architecture The general architecture is shown above. Its composed of four main components: DataStream , Process , Collector , and Analyzer . Each component will be discussed in further detail in the subsequent subsections. Roadmap PyMMDT is still under-development, but we hope that with more effort, time and code contributions, PyMMDT becomes an empowering tool for multimodal data collection, processing, and visualization. Below include features that we hope to include soon: Create UI tool Online PyMMDT implementation Online Collector Online Data Stream Sensor Audio subpackage Code-Level TODOs: - Use isort for import handling - Use black for automatic reformat code - Create repo badges to track codebase quality","title":"PyMMDT"},{"location":"#pymmdt","text":"Welcome to PyMMDT documentation! The GO-TO spot for easy and modular multimodal analysis and visualization.","title":"PyMMDT"},{"location":"#what-is-pymmdt","text":"The objective of PyMMDT library is to make multimodal data collection, processing, and visualization for both the offline and online setting an easier task. Our modular structure helps make this library flexible to be molded and shaped to your project's needs. PyMDDT General Architecture The general architecture is shown above. Its composed of four main components: DataStream , Process , Collector , and Analyzer . Each component will be discussed in further detail in the subsequent subsections.","title":"What is PyMMDT?"},{"location":"#roadmap","text":"PyMMDT is still under-development, but we hope that with more effort, time and code contributions, PyMMDT becomes an empowering tool for multimodal data collection, processing, and visualization. Below include features that we hope to include soon: Create UI tool Online PyMMDT implementation Online Collector Online Data Stream Sensor Audio subpackage Code-Level TODOs: - Use isort for import handling - Use black for automatic reformat code - Create repo badges to track codebase quality","title":"Roadmap"},{"location":"about/","text":"About PyMMDT PyMMDT is a toolkit for Multimodal Data analytics and visualization. It provides a modular design - where computational processes can be added and removed. License PyMMDT uses the GNU GENERAL PUBLIC LICENSE, as found in LICENSE file.","title":"About PyMMDT"},{"location":"about/#about-pymmdt","text":"PyMMDT is a toolkit for Multimodal Data analytics and visualization. It provides a modular design - where computational processes can be added and removed.","title":"About PyMMDT"},{"location":"about/#license","text":"PyMMDT uses the GNU GENERAL PUBLIC LICENSE, as found in LICENSE file.","title":"License"},{"location":"basics/","text":"Basics The architecture can be broken down to two main components: before and after the Collector . Before the Collector the data is handled indepedent of each other - in a unimodal fashion. After the integration of the Collector , multimodal processing can occur. This is done by the use of a global timetrack that helps determine the time order of data from various data sources. The Analyzer then takes the chronologically ordered data from the collector and propagates data samples through their respective data pipelines. At the end, the analyzer uses the Session instance to store the generated multimodal data streams. Data Stream The DataStream class is a fundamental building for this library. It contains the necessary methods and attributes to help organize the data modalities. Process The Process class is a compartmentalized computation operation that is applied to input data streams. A process can be used for both the indepedent pre-fusion or mixed post-fusion processing stages. Collector The Collector class has one goal: align the timing between the incoming data streams and provide data samples to the Analyzer in a chronological form. Analyzer The Analyzer class is the main component of the pipeline. The Analyzer constructs the post-fusion data pipelines for each type of data stream in the Collector . As a new data sample is passed from the Collector to the Analyzer , the Analyzer feeds the sample to its respective pipeline and stores intermediate and final data points in its Session attribute. Modality-Specific Toolkits The goal of PyMMDT is to provide general toolkits for each type of popular data modality. Currently, only video and tabular logs toolkits are available in mm.video and mm.logs subpackages. Video The video subpackage is home for useful implementations of DataStream and Process classes that are specially catered to video data. Tabular Logs The tabular logs subpackage is focused on CSV, Excel and other tabular forms of log data. These include implementations of DataStream with additional useful methods.","title":"Basics"},{"location":"basics/#basics","text":"The architecture can be broken down to two main components: before and after the Collector . Before the Collector the data is handled indepedent of each other - in a unimodal fashion. After the integration of the Collector , multimodal processing can occur. This is done by the use of a global timetrack that helps determine the time order of data from various data sources. The Analyzer then takes the chronologically ordered data from the collector and propagates data samples through their respective data pipelines. At the end, the analyzer uses the Session instance to store the generated multimodal data streams.","title":"Basics"},{"location":"basics/#data-stream","text":"The DataStream class is a fundamental building for this library. It contains the necessary methods and attributes to help organize the data modalities.","title":"Data Stream"},{"location":"basics/#process","text":"The Process class is a compartmentalized computation operation that is applied to input data streams. A process can be used for both the indepedent pre-fusion or mixed post-fusion processing stages.","title":"Process"},{"location":"basics/#collector","text":"The Collector class has one goal: align the timing between the incoming data streams and provide data samples to the Analyzer in a chronological form.","title":"Collector"},{"location":"basics/#analyzer","text":"The Analyzer class is the main component of the pipeline. The Analyzer constructs the post-fusion data pipelines for each type of data stream in the Collector . As a new data sample is passed from the Collector to the Analyzer , the Analyzer feeds the sample to its respective pipeline and stores intermediate and final data points in its Session attribute.","title":"Analyzer"},{"location":"basics/#modality-specific-toolkits","text":"The goal of PyMMDT is to provide general toolkits for each type of popular data modality. Currently, only video and tabular logs toolkits are available in mm.video and mm.logs subpackages.","title":"Modality-Specific Toolkits"},{"location":"basics/#video","text":"The video subpackage is home for useful implementations of DataStream and Process classes that are specially catered to video data.","title":"Video"},{"location":"basics/#tabular-logs","text":"The tabular logs subpackage is focused on CSV, Excel and other tabular forms of log data. These include implementations of DataStream with additional useful methods.","title":"Tabular Logs"},{"location":"installation/","text":"Installation Currently as PyMMDT is under-development, the only installation option is from source. git clone https://github.com/edavalosanaya/PyMMDT cd PyMMDT python -m setup.py","title":"Installation"},{"location":"installation/#installation","text":"Currently as PyMMDT is under-development, the only installation option is from source. git clone https://github.com/edavalosanaya/PyMMDT cd PyMMDT python -m setup.py","title":"Installation"},{"location":"tutorials/","text":"Tutorials","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"api-docs/analyzer/","text":"module analyzer Module focused on the Analyzer implementation. Contains the following classes: Analyzer class Analyzer Multimodal Data Processing and Analysis Coordinator. The Analyzer is the coordinator between the other modules. First, the analyzer determines the flow of data from the data streams stored in the collector and the given processes. Once the analyzer constructs the data flow graph, the pipelines for the input data stream are determined and stored. Then, when feeding new data samples, the analyzer send them to the right pipeline and its processes. The original data sample and its subsequent generated data sample in the pipeline are stored in the Session as a means to keep track of the latest sample. Attributes: collector (pymddt.Collector): The collector used to match the timetracks of each individual data stream. processes (Sequence[pymddt.Process]): A list of processes to be executed depending on their inputs and triggers. session (pymddt.Session): The session that stores all of the latest data samples from original and generated data streams. data_flow_graph (nx.DiGraph): The data flow constructed from the data streams and the processes. pipeline_lookup (Dict): A dictionary that stores the pipelines for each type of input data stream. Todo: * Make the session have the option to save intermediate data sample during the analysis, if the user request this feature. method __init__ __init__( collector: Union[Collector, OfflineCollector], processes: Sequence[Process], session: Session ) \u2192 None Construct the analyzer. Args: collector (pymddt.Collector): The collector used to match the timetracks of each individual data stream. processes (Sequence[pymddt.Process]): A list of processes to be executed depending on their inputs and triggers. session (pymddt.Session): The session that stores all of the latest data samples from original and generated data streams. method close close() \u2192 None Close routine that executes all other's closing routines. The Analyzer 's closing routine closes all the Process , Collector , and DataStream by executing .close() methods in each respective component. method get_sample_pipeline get_sample_pipeline(sample: DataSample) \u2192 Sequence[Process] Get the processes that are dependent to this type of data sample. Args: sample (pymddt.DataSample): The data sample that contains the data type used to select the data pipeline. method step step(sample: DataSample) \u2192 None Routine executed to process a data sample. Given a DataSample from one of the DataStream s in the Collector , the Analyzer propagates the data sample through its respective data pipeline and saves intermediate and final data samples into its Session attribute. Args: sample (pymddt.DataSample): The new input data sample to will be propagated though its corresponding pipeline and stored. This file was automatically generated via lazydocs .","title":"Analyzer"},{"location":"api-docs/analyzer/#module-analyzer","text":"Module focused on the Analyzer implementation. Contains the following classes: Analyzer","title":"module analyzer"},{"location":"api-docs/analyzer/#class-analyzer","text":"Multimodal Data Processing and Analysis Coordinator. The Analyzer is the coordinator between the other modules. First, the analyzer determines the flow of data from the data streams stored in the collector and the given processes. Once the analyzer constructs the data flow graph, the pipelines for the input data stream are determined and stored. Then, when feeding new data samples, the analyzer send them to the right pipeline and its processes. The original data sample and its subsequent generated data sample in the pipeline are stored in the Session as a means to keep track of the latest sample. Attributes: collector (pymddt.Collector): The collector used to match the timetracks of each individual data stream. processes (Sequence[pymddt.Process]): A list of processes to be executed depending on their inputs and triggers. session (pymddt.Session): The session that stores all of the latest data samples from original and generated data streams. data_flow_graph (nx.DiGraph): The data flow constructed from the data streams and the processes. pipeline_lookup (Dict): A dictionary that stores the pipelines for each type of input data stream. Todo: * Make the session have the option to save intermediate data sample during the analysis, if the user request this feature.","title":"class Analyzer"},{"location":"api-docs/analyzer/#method-__init__","text":"__init__( collector: Union[Collector, OfflineCollector], processes: Sequence[Process], session: Session ) \u2192 None Construct the analyzer. Args: collector (pymddt.Collector): The collector used to match the timetracks of each individual data stream. processes (Sequence[pymddt.Process]): A list of processes to be executed depending on their inputs and triggers. session (pymddt.Session): The session that stores all of the latest data samples from original and generated data streams.","title":"method __init__"},{"location":"api-docs/analyzer/#method-close","text":"close() \u2192 None Close routine that executes all other's closing routines. The Analyzer 's closing routine closes all the Process , Collector , and DataStream by executing .close() methods in each respective component.","title":"method close"},{"location":"api-docs/analyzer/#method-get_sample_pipeline","text":"get_sample_pipeline(sample: DataSample) \u2192 Sequence[Process] Get the processes that are dependent to this type of data sample. Args: sample (pymddt.DataSample): The data sample that contains the data type used to select the data pipeline.","title":"method get_sample_pipeline"},{"location":"api-docs/analyzer/#method-step","text":"step(sample: DataSample) \u2192 None Routine executed to process a data sample. Given a DataSample from one of the DataStream s in the Collector , the Analyzer propagates the data sample through its respective data pipeline and saves intermediate and final data samples into its Session attribute. Args: sample (pymddt.DataSample): The new input data sample to will be propagated though its corresponding pipeline and stored. This file was automatically generated via lazydocs .","title":"method step"},{"location":"api-docs/collector/","text":"module collector Module focused on the Collector and its various implementations. Contains the following classes: Collector OfflineCollector OnlineCollector class Collector Generic collector that stores a data streams. Attributes: data_streams (Dict[str, pymddt.DataStream]): A dictionary of the data streams that its keys are the name of the data streams. method __init__ __init__(data_streams: Sequence[DataStream]) \u2192 None Construct the Collector . Args: data_streams (List[pymddt.DataStream]): A list of data streams. class OfflineCollector Generic collector that stores only offline data streams. The offline collector allows the use of both getitem and next to obtain the data pointer to a data stream to fetch the actual data. Attributes: data_streams (Dict[str, pymddt.OfflineDataStream]): A dictionary of the data streams that its keys are the name of the data streams. global_timetrack (pd.DataFrame): A data frame that stores the time, data stream type, and data pointers to allow the iteration over all samples in all data streams efficiently. method __init__ __init__(data_streams: Sequence[OfflineDataStream]) \u2192 None Construct the OfflineCollector . Args: data_streams (List[pymddt.OfflineDataStream]): A list of offline data streams. class OnlineCollector TODO implementation. method __init__ __init__(data_streams: Sequence[DataStream]) \u2192 None Construct the Collector . Args: data_streams (List[pymddt.DataStream]): A list of data streams. This file was automatically generated via lazydocs .","title":"Collector"},{"location":"api-docs/collector/#module-collector","text":"Module focused on the Collector and its various implementations. Contains the following classes: Collector OfflineCollector OnlineCollector","title":"module collector"},{"location":"api-docs/collector/#class-collector","text":"Generic collector that stores a data streams. Attributes: data_streams (Dict[str, pymddt.DataStream]): A dictionary of the data streams that its keys are the name of the data streams.","title":"class Collector"},{"location":"api-docs/collector/#method-__init__","text":"__init__(data_streams: Sequence[DataStream]) \u2192 None Construct the Collector . Args: data_streams (List[pymddt.DataStream]): A list of data streams.","title":"method __init__"},{"location":"api-docs/collector/#class-offlinecollector","text":"Generic collector that stores only offline data streams. The offline collector allows the use of both getitem and next to obtain the data pointer to a data stream to fetch the actual data. Attributes: data_streams (Dict[str, pymddt.OfflineDataStream]): A dictionary of the data streams that its keys are the name of the data streams. global_timetrack (pd.DataFrame): A data frame that stores the time, data stream type, and data pointers to allow the iteration over all samples in all data streams efficiently.","title":"class OfflineCollector"},{"location":"api-docs/collector/#method-__init___1","text":"__init__(data_streams: Sequence[OfflineDataStream]) \u2192 None Construct the OfflineCollector . Args: data_streams (List[pymddt.OfflineDataStream]): A list of offline data streams.","title":"method __init__"},{"location":"api-docs/collector/#class-onlinecollector","text":"TODO implementation.","title":"class OnlineCollector"},{"location":"api-docs/collector/#method-__init___2","text":"__init__(data_streams: Sequence[DataStream]) \u2192 None Construct the Collector . Args: data_streams (List[pymddt.DataStream]): A list of data streams. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api-docs/data_sample/","text":"module data_sample Module focused on DataSample implementation. Contains the following classes: DataSample class DataSample Standard output data type for Data Streams. Attributes: dtype (str): Typically the name of the parent DataStream. data (Any): The data content of the sample. time (pd.Timestamp): the timestamp associated with the sample. method __init__ __init__(dtype: str, time: Timestamp, data) \u2192 None Construct the DataSample . Args: dtype (str): Typically the name of the parent DataStream. data (Any): The data content of the sample. time (pd.Timestamp): the timestamp associated with the sample. This file was automatically generated via lazydocs .","title":"Data sample"},{"location":"api-docs/data_sample/#module-data_sample","text":"Module focused on DataSample implementation. Contains the following classes: DataSample","title":"module data_sample"},{"location":"api-docs/data_sample/#class-datasample","text":"Standard output data type for Data Streams. Attributes: dtype (str): Typically the name of the parent DataStream. data (Any): The data content of the sample. time (pd.Timestamp): the timestamp associated with the sample.","title":"class DataSample"},{"location":"api-docs/data_sample/#method-__init__","text":"__init__(dtype: str, time: Timestamp, data) \u2192 None Construct the DataSample . Args: dtype (str): Typically the name of the parent DataStream. data (Any): The data content of the sample. time (pd.Timestamp): the timestamp associated with the sample. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api-docs/data_source/","text":"module data_source Module focused on the DataSource implementations. Contains the following classes: DataSource . Sensor . API . class DataSource TODO Implementation. class Sensor TODO Implementation. class Api TODO Implementation. This file was automatically generated via lazydocs .","title":"Data source"},{"location":"api-docs/data_source/#module-data_source","text":"Module focused on the DataSource implementations. Contains the following classes: DataSource . Sensor . API .","title":"module data_source"},{"location":"api-docs/data_source/#class-datasource","text":"TODO Implementation.","title":"class DataSource"},{"location":"api-docs/data_source/#class-sensor","text":"TODO Implementation.","title":"class Sensor"},{"location":"api-docs/data_source/#class-api","text":"TODO Implementation. This file was automatically generated via lazydocs .","title":"class Api"},{"location":"api-docs/data_stream/","text":"module data_stream Module focus on DataStream and its various implementations. Contains the following classes: DataStream OfflineDataStream OnlineDataStream class DataStream Generic data sample for both offline and online streaming. The DataStream class is a generic class that needs to be inherented and have its iter and next methods overwritten. Raises: NotImplementedError : The iter and next functions need to be implemented before calling an instance of this class. method __init__ __init__(name: str) \u2192 None Construct the DataStream . Args: name (str): the name of the data stream. method close close() \u2192 None Close routine for DataStream . class OfflineDataStream Generic data stream for offline processing. Mostly loading data files. OfflineDataStream is intended to be inherented and its getitem method to be overwritten to fit the modality of the actual data. Attributes: index (int): Keeping track of the current sample to load in next . Raises: NotImplementedError : getitem function needs to be implemented before calling. method __init__ __init__(name: str, timetrack: DataFrame) Construct the OfflineDataStream. Args: name (str): the name of the data stream. timetrack (pd.DataFrame): the time track of the data stream where timestamps are provided for each data point. method close close() \u2192 None Close routine for DataStream . method set_index set_index(new_index: int) \u2192 None Set the index used for the next method. Args: new_index (int): The new index to be set. method trim_after trim_after(trim_time: Timestamp) \u2192 None Remove data points after the trim_time timestamp. Args: trim_time (pd.Timestamp): The cut-off time. method trim_before trim_before(trim_time: Timestamp) \u2192 None Remove data points before the trim_time timestamp. Args: trim_time (pd.Timestamp): The cut-off time. This file was automatically generated via lazydocs .","title":"Data stream"},{"location":"api-docs/data_stream/#module-data_stream","text":"Module focus on DataStream and its various implementations. Contains the following classes: DataStream OfflineDataStream OnlineDataStream","title":"module data_stream"},{"location":"api-docs/data_stream/#class-datastream","text":"Generic data sample for both offline and online streaming. The DataStream class is a generic class that needs to be inherented and have its iter and next methods overwritten. Raises: NotImplementedError : The iter and next functions need to be implemented before calling an instance of this class.","title":"class DataStream"},{"location":"api-docs/data_stream/#method-__init__","text":"__init__(name: str) \u2192 None Construct the DataStream . Args: name (str): the name of the data stream.","title":"method __init__"},{"location":"api-docs/data_stream/#method-close","text":"close() \u2192 None Close routine for DataStream .","title":"method close"},{"location":"api-docs/data_stream/#class-offlinedatastream","text":"Generic data stream for offline processing. Mostly loading data files. OfflineDataStream is intended to be inherented and its getitem method to be overwritten to fit the modality of the actual data. Attributes: index (int): Keeping track of the current sample to load in next . Raises: NotImplementedError : getitem function needs to be implemented before calling.","title":"class OfflineDataStream"},{"location":"api-docs/data_stream/#method-__init___1","text":"__init__(name: str, timetrack: DataFrame) Construct the OfflineDataStream. Args: name (str): the name of the data stream. timetrack (pd.DataFrame): the time track of the data stream where timestamps are provided for each data point.","title":"method __init__"},{"location":"api-docs/data_stream/#method-close_1","text":"close() \u2192 None Close routine for DataStream .","title":"method close"},{"location":"api-docs/data_stream/#method-set_index","text":"set_index(new_index: int) \u2192 None Set the index used for the next method. Args: new_index (int): The new index to be set.","title":"method set_index"},{"location":"api-docs/data_stream/#method-trim_after","text":"trim_after(trim_time: Timestamp) \u2192 None Remove data points after the trim_time timestamp. Args: trim_time (pd.Timestamp): The cut-off time.","title":"method trim_after"},{"location":"api-docs/data_stream/#method-trim_before","text":"trim_before(trim_time: Timestamp) \u2192 None Remove data points before the trim_time timestamp. Args: trim_time (pd.Timestamp): The cut-off time. This file was automatically generated via lazydocs .","title":"method trim_before"},{"location":"api-docs/process/","text":"module process Module focused on the Process implementation. Contains the following classes: Process class MetaProcess A meta class to ensure that the output of the process is a DataSample. Information: https://stackoverflow.com/questions/57104276/python-subclass-method-to-inherit-decorator-from-superclass-method class Process Generic class that compartmentalizes computational steps for a datastream. Attributes: inputs (List[str]): A list of strings that specific what type of data stream inputs are needed to compute. The order in which they are provided imply the order in the arguments of the forward method. Whenever a new data sample is obtain for the input, this process is executed. output (Optional[str]): The name used to store the output of the forward method. trigger (Optional[str]): An optional parameter that overwrites the inputs as the trigger. Instead of executing this process everytime there is a new data sample for the input, it now only executes this process when a new sample with the data_type of the trigger is obtain. method __init__ __init__( inputs: List[str], output: Optional[str] = None, trigger: Optional[str] = None ) Construct the Process . Args: inputs (List[str]): A list of strings that specific what type of data stream inputs are needed to compute. The order in which they are provided imply the order in the arguments of the forward method. Whenever a new data sample is obtain for the input, this process is executed. output (Optional[str]): The name used to store the output of the forward method. trigger (Optional[str]): An optional parameter that overwrites the inputs as the trigger. Instead of executing this process everytime there is a new data sample for the input, it now only executes this process when a new sample with the data_type of the trigger is obtain. method close close() Close function performed to close the process. function wrapper wrapper(*args, **kwargs) This file was automatically generated via lazydocs .","title":"Process"},{"location":"api-docs/process/#module-process","text":"Module focused on the Process implementation. Contains the following classes: Process","title":"module process"},{"location":"api-docs/process/#class-metaprocess","text":"A meta class to ensure that the output of the process is a DataSample. Information: https://stackoverflow.com/questions/57104276/python-subclass-method-to-inherit-decorator-from-superclass-method","title":"class MetaProcess"},{"location":"api-docs/process/#class-process","text":"Generic class that compartmentalizes computational steps for a datastream. Attributes: inputs (List[str]): A list of strings that specific what type of data stream inputs are needed to compute. The order in which they are provided imply the order in the arguments of the forward method. Whenever a new data sample is obtain for the input, this process is executed. output (Optional[str]): The name used to store the output of the forward method. trigger (Optional[str]): An optional parameter that overwrites the inputs as the trigger. Instead of executing this process everytime there is a new data sample for the input, it now only executes this process when a new sample with the data_type of the trigger is obtain.","title":"class Process"},{"location":"api-docs/process/#method-__init__","text":"__init__( inputs: List[str], output: Optional[str] = None, trigger: Optional[str] = None ) Construct the Process . Args: inputs (List[str]): A list of strings that specific what type of data stream inputs are needed to compute. The order in which they are provided imply the order in the arguments of the forward method. Whenever a new data sample is obtain for the input, this process is executed. output (Optional[str]): The name used to store the output of the forward method. trigger (Optional[str]): An optional parameter that overwrites the inputs as the trigger. Instead of executing this process everytime there is a new data sample for the input, it now only executes this process when a new sample with the data_type of the trigger is obtain.","title":"method __init__"},{"location":"api-docs/process/#method-close","text":"close() Close function performed to close the process.","title":"method close"},{"location":"api-docs/process/#function-wrapper","text":"wrapper(*args, **kwargs) This file was automatically generated via lazydocs .","title":"function wrapper"},{"location":"api-docs/session/","text":"module session Module focused on the Session implementation. Contains the following classes: Session class Session Data Storage that contains the latest version of all data types. Attributes: records (Dict[str, DataSample]): Stores the latest version of a data_type sample or the output of a process. Todo: * Allow the option to store the intermediate samples stored in the session. method __init__ __init__() \u2192 None Session Constructor. method apply apply(process: Process) \u2192 Optional[DataSample] Apply the process by obtaining the necessary inputs and stores the generated output in the records. Args: process (pymddt.Process): The process to be executed with the records. method close close() \u2192 None Close session. Todo: * Add an argument to session to allow the saving of the session values at the end. method update update(sample: DataSample) \u2192 None Store the sample into the records. Args: sample (pymddt.DataSample): The sample to be stored in the records. This file was automatically generated via lazydocs .","title":"Session"},{"location":"api-docs/session/#module-session","text":"Module focused on the Session implementation. Contains the following classes: Session","title":"module session"},{"location":"api-docs/session/#class-session","text":"Data Storage that contains the latest version of all data types. Attributes: records (Dict[str, DataSample]): Stores the latest version of a data_type sample or the output of a process. Todo: * Allow the option to store the intermediate samples stored in the session.","title":"class Session"},{"location":"api-docs/session/#method-__init__","text":"__init__() \u2192 None Session Constructor.","title":"method __init__"},{"location":"api-docs/session/#method-apply","text":"apply(process: Process) \u2192 Optional[DataSample] Apply the process by obtaining the necessary inputs and stores the generated output in the records. Args: process (pymddt.Process): The process to be executed with the records.","title":"method apply"},{"location":"api-docs/session/#method-close","text":"close() \u2192 None Close session. Todo: * Add an argument to session to allow the saving of the session values at the end.","title":"method close"},{"location":"api-docs/session/#method-update","text":"update(sample: DataSample) \u2192 None Store the sample into the records. Args: sample (pymddt.DataSample): The sample to be stored in the records. This file was automatically generated via lazydocs .","title":"method update"},{"location":"api-docs/tabular/","text":"module tabular Tabular Subpackage. This subpackage contains special implementations for the following: - DataStream - OfflineTabularDataStream Global Variables tabular_data_stream This file was automatically generated via lazydocs .","title":"Tabular"},{"location":"api-docs/tabular/#module-tabular","text":"Tabular Subpackage. This subpackage contains special implementations for the following: - DataStream - OfflineTabularDataStream","title":"module tabular"},{"location":"api-docs/tabular/#global-variables","text":"tabular_data_stream This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/tabular.tabular_data_stream/","text":"module tabular.tabular_data_stream Module focused on Tabular Data Stream implementation. Contains the following classes: OfflineTabularDataStream class OfflineTabularDataStream Implementation of Offline DataStream focused on Tabular data. Attributes: name (str): The name of the data stream. data (pd.DataFrame): The loaded Tabular data in pd.DataFrame form. data_columns (Sequence[str]): A list of string containing the name of the data columns to select from. method __init__ __init__( name: str, data: DataFrame, time_column: str, data_columns: Sequence[str] ) Construct ``OffineTabularDataStream. Args: name (str): The name of the data stream. data (pd.DataFrame): The loaded Tabular data in pd.DataFrame form. time_column (str): The column within the data that has the time data. data_columns (Sequence[str]): A list of string containing the name of the data columns to select from. classmethod from_process_and_ds from_process_and_ds( process: Process, in_ds: OfflineDataStream, verbose: bool = False ) Class method to construct data stream from an applied process to a data stream. Args: process (Process): the applied process. in_ds (OfflineTabularDataStream): the incoming data stream to be processed. Returns: self (OfflineTabularDataStream): the generated data stream. This file was automatically generated via lazydocs .","title":"Tabular.tabular data stream"},{"location":"api-docs/tabular.tabular_data_stream/#module-tabulartabular_data_stream","text":"Module focused on Tabular Data Stream implementation. Contains the following classes: OfflineTabularDataStream","title":"module tabular.tabular_data_stream"},{"location":"api-docs/tabular.tabular_data_stream/#class-offlinetabulardatastream","text":"Implementation of Offline DataStream focused on Tabular data. Attributes: name (str): The name of the data stream. data (pd.DataFrame): The loaded Tabular data in pd.DataFrame form. data_columns (Sequence[str]): A list of string containing the name of the data columns to select from.","title":"class OfflineTabularDataStream"},{"location":"api-docs/tabular.tabular_data_stream/#method-__init__","text":"__init__( name: str, data: DataFrame, time_column: str, data_columns: Sequence[str] ) Construct ``OffineTabularDataStream. Args: name (str): The name of the data stream. data (pd.DataFrame): The loaded Tabular data in pd.DataFrame form. time_column (str): The column within the data that has the time data. data_columns (Sequence[str]): A list of string containing the name of the data columns to select from.","title":"method __init__"},{"location":"api-docs/tabular.tabular_data_stream/#classmethod-from_process_and_ds","text":"from_process_and_ds( process: Process, in_ds: OfflineDataStream, verbose: bool = False ) Class method to construct data stream from an applied process to a data stream. Args: process (Process): the applied process. in_ds (OfflineTabularDataStream): the incoming data stream to be processed. Returns: self (OfflineTabularDataStream): the generated data stream. This file was automatically generated via lazydocs .","title":"classmethod from_process_and_ds"},{"location":"api-docs/video/","text":"module video Video Subpackage. This subpackage contains special implementations for the following: - DataStream - OfflineVideoDataStream - Process - ShowVideo Global Variables video_data_stream processes This file was automatically generated via lazydocs .","title":"Video"},{"location":"api-docs/video/#module-video","text":"Video Subpackage. This subpackage contains special implementations for the following: - DataStream - OfflineVideoDataStream - Process - ShowVideo","title":"module video"},{"location":"api-docs/video/#global-variables","text":"video_data_stream processes This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/video.processes/","text":"module video.processes Module focused on video process implementations. Contains the following classes: ShowVideo SaveVideo class ShowVideo Basic process that shows the video in a CV window. Attributes: inputs (Sequence[str]): A list of strings containing the inputs requred to execute ShowVideo . In this case, it needs a video frame. ms_delay (int): A millisecond delay between shown frame. method __init__ __init__(inputs: Sequence[str], ms_delay: int = 1) Construct new ShowVideo instance. Args: inputs (Sequence[str]): A list of strings containing the inputs required to execute ShowVideo . In this case, it needs a video frame. ms_delay (int): A millisecond delay between shown frame. class SaveVideo Basic process that saves the video. Attributes: inputs (Sequence[str]): A list of strings containing the inputs required to execute SaveVideo . In this case, it needs a video frame. fps (int): The frames per second (FPS) used to save the video. size (Tuple[int, int]): The width and height of the video. writer (cv2.VideoWriter): The video writer from OpenCV used to write and save the video. method __init__ __init__( inputs: Sequence[str], filepath: Union[str, Path], fps: int, size: Tuple[int, int], trigger: Optional[str] = None ) \u2192 None Construct new SaveVideo instance. Args: inputs (Sequence[str]): A list of strings containing the inputs required to execute SaveVideo . In this case, it needs a video frame. filepath (Union[str, pathlib.Path]): The filepath to save the new video file. fps (int): The frames per second (FPS) used to save the video. size (Tuple[int, int]): The width and height of the video. trigger (Optional[str]): The possible trigger to save the video, instead of relying on the inputs' update. method close close() \u2192 None Close the video writer and save the video. This file was automatically generated via lazydocs .","title":"Video.processes"},{"location":"api-docs/video.processes/#module-videoprocesses","text":"Module focused on video process implementations. Contains the following classes: ShowVideo SaveVideo","title":"module video.processes"},{"location":"api-docs/video.processes/#class-showvideo","text":"Basic process that shows the video in a CV window. Attributes: inputs (Sequence[str]): A list of strings containing the inputs requred to execute ShowVideo . In this case, it needs a video frame. ms_delay (int): A millisecond delay between shown frame.","title":"class ShowVideo"},{"location":"api-docs/video.processes/#method-__init__","text":"__init__(inputs: Sequence[str], ms_delay: int = 1) Construct new ShowVideo instance. Args: inputs (Sequence[str]): A list of strings containing the inputs required to execute ShowVideo . In this case, it needs a video frame. ms_delay (int): A millisecond delay between shown frame.","title":"method __init__"},{"location":"api-docs/video.processes/#class-savevideo","text":"Basic process that saves the video. Attributes: inputs (Sequence[str]): A list of strings containing the inputs required to execute SaveVideo . In this case, it needs a video frame. fps (int): The frames per second (FPS) used to save the video. size (Tuple[int, int]): The width and height of the video. writer (cv2.VideoWriter): The video writer from OpenCV used to write and save the video.","title":"class SaveVideo"},{"location":"api-docs/video.processes/#method-__init___1","text":"__init__( inputs: Sequence[str], filepath: Union[str, Path], fps: int, size: Tuple[int, int], trigger: Optional[str] = None ) \u2192 None Construct new SaveVideo instance. Args: inputs (Sequence[str]): A list of strings containing the inputs required to execute SaveVideo . In this case, it needs a video frame. filepath (Union[str, pathlib.Path]): The filepath to save the new video file. fps (int): The frames per second (FPS) used to save the video. size (Tuple[int, int]): The width and height of the video. trigger (Optional[str]): The possible trigger to save the video, instead of relying on the inputs' update.","title":"method __init__"},{"location":"api-docs/video.processes/#method-close","text":"close() \u2192 None Close the video writer and save the video. This file was automatically generated via lazydocs .","title":"method close"},{"location":"api-docs/video.video_data_stream/","text":"module video.video_data_stream Module focused on Video Data Streams. Contains the following classes: OfflineVideoDataStream class OfflineVideoDataStream Implementation of Offline DataStream focused on Video data. Attributes: name (str): The name of the data stream. video_path (Union[pathlib.Path, str]): The path to the video file. start_time (pd.Timestamp): The timestamp used to dictate the beginning of the video. method __init__ __init__(name: str, video_path: Union[Path, str], start_time: Timestamp) \u2192 None Construct new OfflineVideoDataStream instance. Args: name (str): The name of the data stream. video_path (Union[pathlib.Path, str]): The path to the video file start_time (pd.Timestamp): The timestamp used to dictate the beginning of the video. method close close() Close the OfflineVideoDataStream instance. method get_size get_size() \u2192 Tuple[int, int] Get the video frame's width and height. Returns: size (Tuple[int, int]): The frame's width and height. method set_index set_index(new_index) Set the video's index by updating the pointer in OpenCV. This file was automatically generated via lazydocs .","title":"Video.video data stream"},{"location":"api-docs/video.video_data_stream/#module-videovideo_data_stream","text":"Module focused on Video Data Streams. Contains the following classes: OfflineVideoDataStream","title":"module video.video_data_stream"},{"location":"api-docs/video.video_data_stream/#class-offlinevideodatastream","text":"Implementation of Offline DataStream focused on Video data. Attributes: name (str): The name of the data stream. video_path (Union[pathlib.Path, str]): The path to the video file. start_time (pd.Timestamp): The timestamp used to dictate the beginning of the video.","title":"class OfflineVideoDataStream"},{"location":"api-docs/video.video_data_stream/#method-__init__","text":"__init__(name: str, video_path: Union[Path, str], start_time: Timestamp) \u2192 None Construct new OfflineVideoDataStream instance. Args: name (str): The name of the data stream. video_path (Union[pathlib.Path, str]): The path to the video file start_time (pd.Timestamp): The timestamp used to dictate the beginning of the video.","title":"method __init__"},{"location":"api-docs/video.video_data_stream/#method-close","text":"close() Close the OfflineVideoDataStream instance.","title":"method close"},{"location":"api-docs/video.video_data_stream/#method-get_size","text":"get_size() \u2192 Tuple[int, int] Get the video frame's width and height. Returns: size (Tuple[int, int]): The frame's width and height.","title":"method get_size"},{"location":"api-docs/video.video_data_stream/#method-set_index","text":"set_index(new_index) Set the video's index by updating the pointer in OpenCV. This file was automatically generated via lazydocs .","title":"method set_index"}]}